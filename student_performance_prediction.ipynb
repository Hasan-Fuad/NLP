{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddd3f426",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import random\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix, classification_report\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# Global visual style upgrade\n",
    "sns.set_theme(style=\"whitegrid\", palette=\"deep\")\n",
    "plt.rcParams[\"figure.figsize\"] = (10, 6)\n",
    "plt.rcParams[\"axes.titlesize\"] = 14\n",
    "plt.rcParams[\"axes.labelsize\"] = 12\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7153b291",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"dataset.csv\")\n",
    "df.shape, df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "283f630d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check missing values\n",
    "df.isnull().sum()\n",
    "\n",
    "# Drop duplicates\n",
    "df = df.drop_duplicates()\n",
    "\n",
    "# Reset index\n",
    "df = df.reset_index(drop=True)\n",
    "\n",
    "df.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce202e56",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Keep only final outcomes\n",
    "df = df[df[\"Target\"].isin([\"Dropout\", \"Graduate\"])]\n",
    "\n",
    "# Convert to binary\n",
    "df[\"pass\"] = df[\"Target\"].map({\n",
    "    \"Dropout\": 0,\n",
    "    \"Graduate\": 1\n",
    "})\n",
    "\n",
    "# Drop original target column\n",
    "df = df.drop(columns=[\"Target\"])\n",
    "\n",
    "df[\"pass\"].value_counts(normalize=True), df.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a6fbede",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.countplot(x=\"pass\", data=df)\n",
    "plt.xticks([0,1],[\"Dropout (Fail)\",\"Graduate (Pass)\"])\n",
    "plt.title(\"Final Student Outcome Distribution\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efb959be",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12,8))\n",
    "corr = df.corr()\n",
    "sns.heatmap(corr, cmap=\"coolwarm\")\n",
    "plt.title(\"Feature Correlation Heatmap\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92b8b36a",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df.drop(columns=[\"pass\"])\n",
    "y = df[\"pass\"]\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42, stratify=y\n",
    ")\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9df964cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "models = {\n",
    "    \"Logistic Regression\": LogisticRegression(class_weight={0:2,1:1}),\n",
    "    \"KNN\": KNeighborsClassifier(),\n",
    "    \"Decision Tree\": DecisionTreeClassifier(class_weight=\"balanced\"),\n",
    "    \"Random Forest\": RandomForestClassifier(class_weight=\"balanced\", n_estimators=300, random_state=42)\n",
    "}\n",
    "\n",
    "results = []\n",
    "\n",
    "for name, model in models.items():\n",
    "    if name in [\"Decision Tree\", \"Random Forest\"]:\n",
    "        model.fit(X_train, y_train)\n",
    "        y_pred = model.predict(X_test)\n",
    "    else:\n",
    "        model.fit(X_train_scaled, y_train)\n",
    "        y_pred = model.predict(X_test_scaled)\n",
    "\n",
    "    results.append({\n",
    "        \"Model\": name,\n",
    "        \"Accuracy\": accuracy_score(y_test, y_pred),\n",
    "        \"Precision\": precision_score(y_test, y_pred),\n",
    "        \"Recall\": recall_score(y_test, y_pred),\n",
    "        \"F1\": f1_score(y_test, y_pred)\n",
    "    })\n",
    "\n",
    "results_df = pd.DataFrame(results)\n",
    "results_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca5a3069",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.barplot(x=\"Model\", y=\"F1\", data=results_df)\n",
    "plt.title(\"Model Comparison (F1-Score)\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f78903b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_model_name = results_df.sort_values(by=\"F1\", ascending=False).iloc[0][\"Model\"]\n",
    "best_model = models[best_model_name]\n",
    "\n",
    "if best_model_name in [\"Decision Tree\", \"Random Forest\"]:\n",
    "    best_model.fit(X_train, y_train)\n",
    "    y_pred_best = best_model.predict(X_test)\n",
    "else:\n",
    "    best_model.fit(X_train_scaled, y_train)\n",
    "    y_pred_best = best_model.predict(X_test_scaled)\n",
    "\n",
    "print(classification_report(y_test, y_pred_best))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afeafbfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "cm = confusion_matrix(y_test, y_pred_best)\n",
    "sns.heatmap(cm, annot=True, fmt=\"d\")\n",
    "plt.title(\"Confusion Matrix\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a02f819a",
   "metadata": {},
   "outputs": [],
   "source": [
    "if best_model_name in [\"Decision Tree\", \"Random Forest\"]:\n",
    "    importances = best_model.feature_importances_\n",
    "else:\n",
    "    rf = RandomForestClassifier(class_weight=\"balanced\", n_estimators=300, random_state=42)\n",
    "    rf.fit(X_train, y_train)\n",
    "    importances = rf.feature_importances_\n",
    "\n",
    "importance_df = pd.DataFrame({\n",
    "    \"Feature\": X.columns,\n",
    "    \"Importance\": importances\n",
    "}).sort_values(by=\"Importance\", ascending=False).head(12)\n",
    "\n",
    "sns.barplot(x=\"Importance\", y=\"Feature\", data=importance_df)\n",
    "plt.title(\"Top 12 Most Important Features\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fe753ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_student_outcome(**inputs):\n",
    "    sample = pd.DataFrame([inputs])\n",
    "\n",
    "    # Align features\n",
    "    sample = sample.reindex(columns=X.columns, fill_value=0)\n",
    "\n",
    "    if best_model_name in [\"Decision Tree\", \"Random Forest\"]:\n",
    "        sample_final = sample\n",
    "    else:\n",
    "        sample_final = scaler.transform(sample)\n",
    "\n",
    "    pred = best_model.predict(sample_final)[0]\n",
    "\n",
    "    if hasattr(best_model, \"predict_proba\"):\n",
    "        prob = best_model.predict_proba(sample_final)[0][pred]\n",
    "    else:\n",
    "        prob = None\n",
    "\n",
    "    result = \"GRADUATE âœ…\" if pred == 1 else \"DROPOUT âŒ\"\n",
    "\n",
    "    print(\"â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\")\n",
    "    print(\"STUDENT RETENTION AI\")\n",
    "    print(\"â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\")\n",
    "    print(\"Prediction:\", result)\n",
    "    if prob is not None:\n",
    "        print(\"Confidence:\", f\"{prob*100:.2f}%\")\n",
    "    print(\"Model:\", best_model_name)\n",
    "    print(\"â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\")\n",
    "\n",
    "    return pred, prob\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40818c36",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_random_student(show=True):\n",
    "    \"\"\"\n",
    "    Generates a random student profile from the dataset\n",
    "    and optionally prints their attributes.\n",
    "    \"\"\"\n",
    "    sample = df.sample(1).drop(columns=[\"pass\"])\n",
    "    student_dict = sample.iloc[0].to_dict()\n",
    "\n",
    "    if show:\n",
    "        print(\"\\nğŸ² RANDOM STUDENT PROFILE\")\n",
    "        print(\"â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\")\n",
    "        for k, v in student_dict.items():\n",
    "            print(f\"{k:45s}: {v}\")\n",
    "        print(\"â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\")\n",
    "\n",
    "    return student_dict\n",
    "\n",
    "\n",
    "def run_simulation(n=10, verbose=True, show_students=True):\n",
    "    \"\"\"\n",
    "    Runs n random student predictions through the trained model.\n",
    "    Shows student profiles, predictions, and returns a summary.\n",
    "    \"\"\"\n",
    "    pass_count = 0\n",
    "    fail_count = 0\n",
    "    results = []\n",
    "\n",
    "    for i in range(n):\n",
    "        print(f\"\\n===== STUDENT {i+1} =====\")\n",
    "        \n",
    "        student = generate_random_student(show=show_students)\n",
    "        pred, prob = predict_student_outcome(**student)\n",
    "\n",
    "        if pred == 1:\n",
    "            pass_count += 1\n",
    "            outcome = \"GRADUATE âœ…\"\n",
    "        else:\n",
    "            fail_count += 1\n",
    "            outcome = \"DROPOUT âŒ\"\n",
    "\n",
    "        results.append({\n",
    "            \"student_id\": i + 1,\n",
    "            \"prediction\": outcome,\n",
    "            \"confidence\": prob,\n",
    "            \"student_profile\": student\n",
    "        })\n",
    "\n",
    "        if verbose:\n",
    "            print(f\"\\nğŸ“Œ MODEL DECISION â†’ {outcome}\")\n",
    "            if prob is not None:\n",
    "                print(f\"ğŸ“Š Confidence â†’ {prob*100:.2f}%\")\n",
    "\n",
    "    summary = {\n",
    "        \"total\": n,\n",
    "        \"predicted_graduates\": pass_count,\n",
    "        \"predicted_dropouts\": fail_count,\n",
    "        \"graduate_rate\": pass_count / n,\n",
    "        \"dropout_rate\": fail_count / n,\n",
    "        \"results\": results\n",
    "    }\n",
    "\n",
    "    print(\"\\nğŸ“Š FINAL SIMULATION SUMMARY\")\n",
    "    print(\"â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\")\n",
    "    print(\"Total students        :\", n)\n",
    "    print(\"Predicted Graduates   :\", pass_count)\n",
    "    print(\"Predicted Dropouts    :\", fail_count)\n",
    "    print(\"Graduate Rate         :\", round(pass_count / n, 3))\n",
    "    print(\"Dropout Rate          :\", round(fail_count / n, 3))\n",
    "    print(\"â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\")\n",
    "\n",
    "    return summary\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "213fbd96",
   "metadata": {},
   "outputs": [],
   "source": [
    "sim_results = run_simulation(n=3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa448cc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def test_output_type():\n",
    "    pred, prob = predict_student_outcome(**generate_random_student())\n",
    "    assert pred in [0, 1]\n",
    "    print(\"âœ… test_output_type passed\")\n",
    "\n",
    "def test_probability_bounds():\n",
    "    pred, prob = predict_student_outcome(**generate_random_student())\n",
    "    if prob is not None:\n",
    "        assert 0 <= prob <= 1\n",
    "    print(\"âœ… test_probability_bounds passed\")\n",
    "\n",
    "def test_extreme_fail_case():\n",
    "    # Construct a profile with very low curricular performance (strong dropout indicators)\n",
    "    student = generate_random_student()\n",
    "    # Set curricular units to 0 (no performance)\n",
    "    for col in X.columns:\n",
    "        if 'Curricular units' in col:\n",
    "            student[col] = 0\n",
    "    pred, _ = predict_student_outcome(**student)\n",
    "    # Just verify we get a prediction, not asserting specific outcome\n",
    "    assert pred in [0, 1]\n",
    "    print(\"âœ… test_extreme_fail_case passed\")\n",
    "\n",
    "def test_extreme_pass_case():\n",
    "    # Construct a profile with excellent curricular performance (strong pass indicators)\n",
    "    student = generate_random_student()\n",
    "    # Set curricular units to high values\n",
    "    for col in X.columns:\n",
    "        if 'approved' in col.lower() or 'grade' in col.lower():\n",
    "            student[col] = X[col].max()\n",
    "    pred, _ = predict_student_outcome(**student)\n",
    "    # Just verify we get a prediction, not asserting specific outcome\n",
    "    assert pred in [0, 1]\n",
    "    print(\"âœ… test_extreme_pass_case passed\")\n",
    "\n",
    "def test_feature_alignment():\n",
    "    student = generate_random_student()\n",
    "    df_test = pd.DataFrame([student])\n",
    "    df_test = df_test.reindex(columns=X.columns, fill_value=0)\n",
    "    assert df_test.shape[1] == X.shape[1]\n",
    "    print(\"âœ… test_feature_alignment passed\")\n",
    "\n",
    "def test_simulation_return_structure():\n",
    "    summary = run_simulation(n=5, verbose=False)\n",
    "\n",
    "    required_keys = {\n",
    "        \"total\", \"predicted_graduates\", \n",
    "        \"predicted_dropouts\", \"graduate_rate\",\n",
    "        \"dropout_rate\", \"results\"\n",
    "    }\n",
    "\n",
    "    assert required_keys.issubset(summary.keys())\n",
    "    assert summary[\"total\"] == 5\n",
    "    print(\"âœ… test_simulation_return_structure passed\")\n",
    "\n",
    "def test_simulation_conservation():\n",
    "    summary = run_simulation(n=8, verbose=False)\n",
    "    total = summary[\"predicted_graduates\"] + summary[\"predicted_dropouts\"]\n",
    "    assert total == 8\n",
    "    print(\"âœ… test_simulation_conservation passed\")\n",
    "\n",
    "def test_multiple_random_predictions_stability():\n",
    "    preds = []\n",
    "    for _ in range(20):\n",
    "        pred, _ = predict_student_outcome(**generate_random_student())\n",
    "        preds.append(pred)\n",
    "\n",
    "    # Ensure both classes appear at least once in multiple runs\n",
    "    assert 0 in preds and 1 in preds\n",
    "    print(\"âœ… test_multiple_random_predictions_stability passed\")\n",
    "\n",
    "# =========================================================\n",
    "# âœ…âœ…âœ… RUN ALL UNIT TESTS\n",
    "# =========================================================\n",
    "\n",
    "def run_all_tests():\n",
    "    print(\"\\nğŸ§ª RUNNING FULL UNIT TEST SUITE...\\n\")\n",
    "    \n",
    "    print(\"Testing output type...\")\n",
    "    test_output_type()\n",
    "    print(\"Testing probability bounds...\")\n",
    "    test_probability_bounds()\n",
    "    print(\"Testing extreme fail case...\")\n",
    "    test_extreme_fail_case()\n",
    "    print(\"Testing extreme pass case...\")\n",
    "    test_extreme_pass_case()\n",
    "    print(\"Testing feature alignment...\")\n",
    "    test_feature_alignment()\n",
    "    print(\"Testing simulation return structure...\")\n",
    "    test_simulation_return_structure()\n",
    "    print(\"Testing simulation conservation...\")\n",
    "    test_simulation_conservation()\n",
    "    print(\"Testing multiple random predictions stability...\")\n",
    "    test_multiple_random_predictions_stability()\n",
    "\n",
    "    print(\"\\nğŸ‰ ALL UNIT TESTS PASSED SUCCESSFULLY! âœ…\")\n",
    "\n",
    "run_all_tests()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
